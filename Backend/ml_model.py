# -*- coding: utf-8 -*-
"""Copy of ML_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HMQlOOoZuZ12-qvVZzwvQDCfhytyV2j1
"""

import subprocess

REQUIRE = False

# List of required modules
required_modules = [
    'pandas',
    'scikit-learn',
    'nltk',
    'IPython',
    'Pillow',
    'numpy'
]

# Install missing modules
if REQUIRE:
  for module in required_modules:
      try:
          __import__(module)
      except ImportError:
          print(f"Installing {module}...")
          subprocess.check_call(["pip", "install", module])

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re
import nltk
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter
from IPython.display import HTML, display
from PIL import Image

if REQUIRE:
  nltk.download('punkt')
  nltk.download('stopwords')
  nltk.download('wordnet')

DEBUG = False
DOWNLOAD = False

# if DOWNLOAD:
#   !wget https://raw.githubusercontent.com/sameermuhd/Recommendation-System/main/Backend/filtered_articles.csv

df_complete = pd.read_csv("filtered_articles.csv", index_col = "article_id")

if DEBUG:
  df_complete.info()

columns_to_keep = ['product_type_name', 'product_group_name', 'graphical_appearance_name', 'colour_group_name',
                   'perceived_colour_value_name', 'perceived_colour_master_name', 'department_name', 'index_name', 'index_group_name',
                   'section_name', 'garment_group_name']

df = df_complete[columns_to_keep]

if DEBUG:
  df.info()

# One-Hot Encoding
one_hot_encoded = pd.get_dummies(df)
if DEBUG:
  one_hot_encoded

# Function to preprocess text
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    # Convert text to lowercase
    text = text.lower()

    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)

    # Tokenize text
    tokens = word_tokenize(text)

    # Remove stopwords
    tokens = [word for word in tokens if word not in stop_words]

    # Lemmatization
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    # Join tokens back into a single string
    preprocessed_text = ' '.join(tokens)

    return preprocessed_text

df_complete['detail_desc'] = df_complete['detail_desc'].astype(str)

# Memory usage before optimization
if DEBUG:
  print("Memory usage before optimization:", df_complete['detail_desc'].memory_usage(deep=True))

# Apply preprocessing to the product_description column
df_complete['detail_desc'] = df_complete['detail_desc'].apply(preprocess_text)

# Memory usage after optimization
if DEBUG:
  print("Memory usage before optimization:", df_complete['detail_desc'].memory_usage(deep=True))

top_most_common_words = 200

# Step 1: Tokenization
tokenized_sentences = df_complete['detail_desc'].apply(lambda x: x.split())

# Step 2: Vocabulary Creation
word_counts = Counter(word for sentence in tokenized_sentences for word in sentence)
vocab = [word for word, _ in word_counts.most_common(top_most_common_words)]  # Keep only top n words

# Step 3 & 4: Word Frequency Calculation and Vector Representation
bag_of_words_vectors = []
for index, tokenized_sentence in tokenized_sentences.items():
    word_counts = Counter(tokenized_sentence)
    vector = [word_counts[word] for word in vocab]
    vector = list(map(lambda num: num >= 1, vector))
    bag_of_words_vectors.append([index] + vector)

# Convert bag_of_words_vectors to DataFrame
bag_of_words_df = pd.DataFrame(bag_of_words_vectors, columns=['article_id'] + vocab)
bag_of_words_df.set_index('article_id', inplace=True)

# Concatenate bag_of_words_df with the original DataFrame df using article_id as index
one_hot_encoded = pd.concat([one_hot_encoded, bag_of_words_df], axis=1)

if DEBUG:
  one_hot_encoded

from sklearn.metrics import jaccard_score
from sklearn.metrics import hamming_loss
from scipy.spatial.distance import hamming

def jaccard_similarity(article_id, other_articles_indices):
    # Get the one-hot encoded vector for the article
    article_vector = one_hot_encoded.loc[article_id].values

    # Get the one-hot encoded vectors for other articles
    other_vectors = one_hot_encoded.loc[other_articles_indices].values

    # Compute Hamming distances
    hamming_distances = np.apply_along_axis(hamming, 1, other_vectors, article_vector)

    # Calculate Jaccard similarities
    similarities = 1 - hamming_distances

    return similarities

# Function to recommend similar products based on input product
def recommend_similar_products(article_id, top_n=10):
    # Find Top n Similar Articles
    similarities = jaccard_similarity(article_id, df[df.index != article_id].index)
    similar_articles = df_complete[df.index != article_id].copy()
    similar_articles['Similarity'] = similarities
    similar_articles = similar_articles.nlargest(top_n, 'Similarity')

    similar_articles['article_id'] = similar_articles.index
    similar_articles['article_id'] = similar_articles['article_id'].apply(np.int64)

    # Convert all columns to strings
    top_similar_products = similar_articles.astype(str)

    return top_similar_products.to_json(orient='records')