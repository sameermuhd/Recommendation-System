# -*- coding: utf-8 -*-
"""Copy of ML_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HMQlOOoZuZ12-qvVZzwvQDCfhytyV2j1
"""

import subprocess

REQUIRE = False

# List of required modules
required_modules = [
    'pandas',
    'scikit-learn',
    'nltk',
    'IPython',
    'Pillow',
    'numpy'
]

# Install missing modules
if REQUIRE:
  for module in required_modules:
      try:
          __import__(module)
      except ImportError:
          print(f"Installing {module}...")
          subprocess.check_call(["pip", "install", module])

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re
import nltk
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter
from IPython.display import HTML, display
from PIL import Image

if REQUIRE:
  nltk.download('punkt')
  nltk.download('stopwords')
  nltk.download('wordnet')

DEBUG = False
DOWNLOAD = False

# if DOWNLOAD:
#   !wget https://raw.githubusercontent.com/sameermuhd/Recommendation-System/main/Backend/Machine-Learning-Model/Dateset/articles.csv

df_complete = pd.read_csv("articles.csv", index_col = "article_id")

if DEBUG:
  df_complete.info()

columns_to_drop = ['prod_name', 'product_type_name', 'graphical_appearance_name', 'product_group_name',
                   'colour_group_name', 'perceived_colour_value_name', 'perceived_colour_master_name',
                   'department_name', 'index_name', 'index_code', 'index_group_name', 'section_name', 'garment_group_name']
df = df_complete.drop(columns=columns_to_drop)
df = df.dropna(subset=['detail_desc'])
if DEBUG:
  df.info()

# Function to preprocess text
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    # Convert text to lowercase
    text = text.lower()

    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)

    # Tokenize text
    tokens = word_tokenize(text)

    # Remove stopwords
    tokens = [word for word in tokens if word not in stop_words]

    # Lemmatization
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    # Join tokens back into a single string
    preprocessed_text = ' '.join(tokens)

    return preprocessed_text

df['detail_desc'] = df['detail_desc'].astype(str)

# Memory usage before optimization
if DEBUG:
  print("Memory usage before optimization:", df['detail_desc'].memory_usage(deep=True))

# Apply preprocessing to the product_description column
df['detail_desc'] = df['detail_desc'].apply(preprocess_text)

# Memory usage after optimization
if DEBUG:
  print("Memory usage before optimization:", df['detail_desc'].memory_usage(deep=True))

top_most_common_words = 200

# Step 1: Tokenization
tokenized_sentences = df['detail_desc'].apply(lambda x: x.split())

# Step 2: Vocabulary Creation
word_counts = Counter(word for sentence in tokenized_sentences for word in sentence)
vocab = [word for word, _ in word_counts.most_common(top_most_common_words)]  # Keep only top n words

# Step 3 & 4: Word Frequency Calculation and Vector Representation
bag_of_words_vectors = []
for index, tokenized_sentence in tokenized_sentences.items():
    word_counts = Counter(tokenized_sentence)
    vector = [word_counts[word] for word in vocab]
    bag_of_words_vectors.append([index] + vector)

# Convert bag_of_words_vectors to DataFrame
bag_of_words_df = pd.DataFrame(bag_of_words_vectors, columns=['article_id'] + vocab)
bag_of_words_df.set_index('article_id', inplace=True)

# Concatenate bag_of_words_df with the original DataFrame df using article_id as index
df = df.drop(columns=['detail_desc'])
df = pd.concat([df, bag_of_words_df], axis=1)
if DEBUG:
  df

# df = df.drop(columns=['detail_desc'])

# Function to recommend similar products based on input product
def recommend_similar_products(article_id, top_n=10):
    # Find features of input product
    product_features = df.loc[article_id].values.reshape(1, -1)

    # Calculate cosine similarity between input product and all other products
    similarities = cosine_similarity(product_features, df)

    # Get indices of top similar products
    similar_indices = similarities.argsort(axis=1)[0][-top_n-1:-1][::-1]

    # Get top similar product details
    top_similar_products = df_complete.iloc[similar_indices]
    top_similar_products['article_id'] = top_similar_products.index
    top_similar_products['article_id'] = top_similar_products['article_id'].apply(np.int64)

    # Convert all columns to strings
    top_similar_products = top_similar_products.astype(str)

    return top_similar_products.to_json(orient='records')